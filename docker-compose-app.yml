services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.4.3
    container_name: vidsage-elasticsearch
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200"]
      interval: 20s
      timeout: 7s
      retries: 5
    networks:
      - vid_network

  ollama:
    image: ollama/ollama
    container_name: vidsage-ollama
    volumes:
      - ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    healthcheck:
      test: ollama list || exit 1
      interval: 10s
      timeout: 30s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia  # use NVIDIA runtime to ensure GPU access
            capabilities: ["gpu"]
            count: all  # adjust count for the number of GPUs you want to use
    networks:
      - vid_network

  ollama-models-pull:
    image: curlimages/curl:8.6.0
    container_name: vidsage-ollama-models-pull
    env_file:
      - .env
    command: >
      sh -c "curl -X POST http://vidsage-ollama:11434/api/pull -d '{\"name\": \"${LLM_MODEL}\"}'"
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - vid_network

  streamlit:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: vidsage-streamlit
    env_file:
      - .env
    ports:
      - "8501:8501"
    volumes:
      - ./app:/app
      - ./app/app_data:/app/app_data
    depends_on:
      elasticsearch:
        condition: service_healthy
      # ollama:
      #   condition: service_healthy
    networks:
    - vid_network

volumes:
  elasticsearch-data:
  ollama-data:

networks:
  vid_network:
    driver: bridge