{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from openai import OpenAI\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "import requests\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>start</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hi everyone so recently I gave a</td>\n",
       "      <td>0.16</td>\n",
       "      <td>4.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30-minute talk on large language models</td>\n",
       "      <td>2.28</td>\n",
       "      <td>4.119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>just kind of like an intro talk um</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text  start  duration\n",
       "0         hi everyone so recently I gave a   0.16     4.080\n",
       "1  30-minute talk on large language models   2.28     4.119\n",
       "2       just kind of like an intro talk um   4.24     4.240"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id = 'zjkBMFhNj_g'\n",
    "srt = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "df_srt = pd.DataFrame(srt)\n",
    "df_srt.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_transcript(df):\n",
    "    out_text =\"\"\n",
    "    for _, row in df.iterrows():\n",
    "        out_text += \" \" + row['text']\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_transcribed = parse_transcript(df_srt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64353,\n",
       " ' hi everyone so recently I gave a 30-minute talk on large language models just kind of like an intro talk um unfortunately that talk was not recorded but a lot of people came to me after the talk and ')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yt_transcribed), yt_transcribed[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Input Blocks | Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blocks(df, block_size=10, stride=3, max_duration=120):\n",
    "    '''\n",
    "    Use sliding window of size 'block_size' minutes with stride of 'stride' minutes to generate text blocks.\n",
    "    Generated blocks wil be limited to 'max_blocks' and can be changed depending upon the processing power.\n",
    "    Default parameters allow videos of upto 2hrs. to be included.\n",
    "    '''\n",
    "    max_blocks = ceil(((max_duration-block_size)/stride)+1)\n",
    "    max_len = ceil(max(df['start'])/60)\n",
    "    df_out = pd.DataFrame()\n",
    "\n",
    "    print(f\"INFO: initiated block creation of video transcript\")\n",
    "    print(f\"INFO: video length {max_len} | block size {block_size} | stride {stride} | total blocks {max_blocks}\")\n",
    "\n",
    "    for i in range(max_blocks):\n",
    "        start = i*stride\n",
    "        stop = block_size + i*stride\n",
    "        df_block = df[(df['start']>= 60*start) & (df['start']<= 60*stop)]\n",
    "        if (i + 1) % 5 == 0 or i + 1 == max_blocks:\n",
    "            print(f\"INFO: generated block {i+1} | start {start} | stop {stop} | rows combined {df_block.shape[0]}\")\n",
    "            print(f\"INFO: reached max blocks limit\")\n",
    "        transcribed = parse_transcript(df_block)\n",
    "        df_block = pd.DataFrame({'Block':[i+1], 'text':[transcribed], 'start_time': [min(df_block['start'])]})\n",
    "        df_out = pd.concat([df_out, df_block])\n",
    "        if stop >= max_len:\n",
    "            print(f\"INFO: generated block {i+1} | start {start} | stop {stop} | rows combined {df_block.shape[0]}\")\n",
    "            print(f\"INFO: reached end of video\")\n",
    "            break\n",
    "    \n",
    "    df_out.reset_index(drop=True, inplace=True)\n",
    "    print(f\"INFO: original data {df.shape} | block data {df_out.shape}\")\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: initiated block creation of video transcript\n",
      "INFO: video length 60 | block size 10 | stride 3 | total blocks 38\n",
      "INFO: generated block 5 | start 12 | stop 22 | rows combined 285\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 10 | start 27 | stop 37 | rows combined 288\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 15 | start 42 | stop 52 | rows combined 278\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 18 | start 51 | stop 61 | rows combined 1\n",
      "INFO: reached end of video\n",
      "INFO: original data (1704, 3) | block data (18, 3)\n"
     ]
    }
   ],
   "source": [
    "df_srt_v2 = create_blocks(df_srt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Block</th>\n",
       "      <th>text</th>\n",
       "      <th>start_time</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hi everyone so recently I gave a 30-minute ta...</td>\n",
       "      <td>0.160</td>\n",
       "      <td>10650</td>\n",
       "      <td>2029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>for example write a poem about the company sc...</td>\n",
       "      <td>180.159</td>\n",
       "      <td>10970</td>\n",
       "      <td>2085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>and so it's kind of like a lossy compression ...</td>\n",
       "      <td>361.479</td>\n",
       "      <td>11064</td>\n",
       "      <td>2067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Block                                               text  start_time  \\\n",
       "0      1   hi everyone so recently I gave a 30-minute ta...       0.160   \n",
       "1      2   for example write a poem about the company sc...     180.159   \n",
       "2      3   and so it's kind of like a lossy compression ...     361.479   \n",
       "\n",
       "   text_length  word_count  \n",
       "0        10650        2029  \n",
       "1        10970        2085  \n",
       "2        11064        2067  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_srt_v2['text_length'] = df_srt_v2['text'].apply(len)\n",
    "df_srt_v2['word_count'] = df_srt_v2['text'].apply(lambda x : len(x.split()))\n",
    "df_srt_v2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n",
    "\n",
    "df_srt_v2['word_count'].plot.hist(bins=12, alpha=0.8, ax=axes[0], title='Word Count')\n",
    "df_srt_v2['text_length'].plot.hist(bins=12, alpha=0.8, ax=axes[1], title='Text Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Clean summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_smry(transcript, llm_model='gemma2:2b'):\n",
    "    '''\n",
    "    This function takes in a 'transcript' text and generates summarized text using 'llm_model' specified.\n",
    "    '''\n",
    "    smrize_prompt = \"\"\"As a professional editor, your task is to convert the provided YouTube transcript into a concise, well-structured summary. Follow below steps:\n",
    "        Steps:\n",
    "        - Clean the text for grammatical correctness and clarity by removing filler words such as 'uhm', 'mhm', and similar phrases.\n",
    "        - Retain as many original phrases as possible for authenticity.\n",
    "        - Generate a summary text that contains all the information from input TRANSCRIPT and don't use words like 'you' and 'I' in the generated summary.\n",
    "        - The summary should be organized into clearly labeled sections and subsections where applicable.\n",
    "        - Ensure that all key information from the input transcript is included in the summary.\n",
    "        - Don't add any new information, don't express your opinions about the speaker and don't suggest any follow up query.\n",
    "        - Do not praise the speaker or me. Just provide the summary as per above directions.\n",
    "        \n",
    "        TRANSCRIPT: {INPUT_TRANSCRIPT}\"\"\"\n",
    "\n",
    "    prompt = smrize_prompt.format(INPUT_TRANSCRIPT = transcript)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        # temperature=0,    # remove randomness for deterministic output but not using it as it makes summary clumsy with phrases like 'you stated correctly...', 'you explained it well...' etc.\n",
    "        seed=72\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hi everyone so recently I gave a 30-minute talk on large language models just kind of like an intro talk um unfortunately that talk was not recorded but a lot of people came to me after the talk and they told me that uh they really liked the talk so I would just I thought I would just re-record it and basically put it up on YouTube so here we go the busy person's intro to large language models director Scott okay so let's begin first of all what is a large language model really well a large language model is just two files right um there will be two files in this hypothetical directory so for example working with a specific example of the Llama 270b model this is a large language model released by meta Ai and this is basically the Llama series of language models the second iteration of it and this is the 70 billion parameter model of uh of this series so there's multiple models uh belonging to the Llama 2 Series uh 7 billion um 13 billion 34 billion and 70 billion is the biggest one now many people like this model specifically because it is probably today the most powerful open weights model so basically the weights and the architecture and a paper was all released by meta so anyone can work with this model very easily uh by themselves uh this is unlike many other language models that you might be familiar with for example if you're using chat GPT or something like that uh the model architecture was never released it is owned by open aai and you're allowed to use the language model through a web interface but you don't have actually access to that model so in this case the Llama 270b model is really just two files on your file system the parameters file and the Run uh some kind of a code that runs those parameters so the parameters are basically the weights or the parameters of this neural network that is the language model we'll go into that in a bit because this is a 70 billion parameter model uh every one of those parameters is stored as 2 bytes and so therefore the parameters file here is 140 gigabytes and it's two bytes because this is a float 16 uh number as the data type now in addition to these parameters that's just like a large list of parameters uh for that neural network you also need something that runs that neural network and this piece of code is implemented in our run file now this could be a C file or a python file or any other programming language really uh it can be written any arbitrary language but C is sort of like a very simple language just to give you a sense and uh it would only require about 500 lines of C with no other dependencies to implement the the uh neural network architecture uh and that uses basically the parameters to run the model so it's only these two files you can take these two files and you can take your MacBook and this is a fully self-contained package this is everything that's necessary you don't need any connectivity to the internet or anything else you can take these two files you compile your C code you get a binary that you can point at the parameters and you can talk to this language model so for example you can send it text like for example write a poem about the company scale Ai and this language model will start generating text and in this case it will follow the directions and give you a poem about scale AI now the reason that I'm picking on scale AI here and you're going to see that throughout the talk is because the event that I originally presented uh this talk with was run by scale Ai and so I'm picking on them throughout uh throughout the slides a little bit just in an effort to make it concrete so this is how we can run the model just requires two files just requires a MacBook I'm slightly cheating here because this was not actually in terms of the speed of this uh video here this was not running a 70 billion parameter model it was only running a 7 billion parameter Model A 70b would be running about 10 times slower but I wanted to give you an idea of uh sort of just the text generation and what that looks like so not a lot is necessary to run the model this is a very small package but the computational complexity really comes in when we'd like to get those parameters so how do we get the parameters and where are they from uh because whatever is in the run. C file um the neural network architecture and sort of the forward pass of that Network everything is algorithmically understood and open and and so on but the magic really is in the parameters and how do we obtain them so to obtain the parameters um basically the model training as we call it is a lot more involved than model inference which is the part that I showed you earlier so model inference is just running it on your MacBook model training is a competition very involved process process so basically what we're doing can best be sort of understood as kind of a compression of a good chunk of Internet so because llama 270b is an open source model we know quite a bit about how it was trained because meta released that information in paper so these are some of the numbers of what's involved you basically take a chunk of the internet that is roughly you should be thinking 10 terab of text this typically comes from like a crawl of the internet so just imagine uh just collecting tons of text from all kinds of different websites and collecting it together so you take a large cheun of internet then you procure a GPU cluster um and uh these are very specialized computers intended for very heavy computational workloads like training of neural networks you need about 6,000 gpus and you would run this for about 12 days uh to get a llama 270b and this would cost you about $2 million and what this is doing is basically it is compressing this uh large chunk of text into what you can think of as a kind of a zip file so these parameters that I showed you in an earlier slide are best kind of thought of as like a zip file of the internet and in this case what would come out are these parameters 140 GB so you can see that the compression ratio here is roughly like 100x uh roughly speaking but this is not exactly a zip file because a zip file is lossless compression What's Happening Here is a lossy compression we're just kind of like getting a kind of a Gestalt of the text that we trained on we don't have an identical copy of it in these parameters and so it's kind of like a lossy compression you can think about it that way the one more thing to point out here is these numbers here are actually by today's standards in terms of state-of-the-art rookie numbers uh so if you want to think about state-of-the-art neural networks like say what you might use in chpt or Claude or Bard or something like that uh these numbers are off by factor of 10 or more so you would just go in then you just like start multiplying um by quite a bit more and that's why these training runs today are many tens or even potentially hundreds of millions of dollars very large clusters very large data sets and this process here is very involved to get those parameters once you have those parameters running the neural network is fairly computationally cheap okay so what is this neural network really doing right I mentioned that there are these parameters um this neural network basically is just trying to predict the next word in a sequence you can think about it that way so you can feed in a sequence of words for example C set on a this feeds into a neural net and these parameters are dispersed throughout this neural network and there's neurons and they're connected to each other and they all fire in a certain way you can think about it that way um and out comes a prediction for what word comes next so for example in this case this neural network might predict that in this context of for Words the next word will probably be a Matt with say 97% probability so this is fundamentally the problem that the neural network is performing and this you can show mathematically that there's a very close relationship between prediction and compression which is why I sort of allude to this neural network as a kind of training it is kind of like a compression of the internet um because if you can predict uh sort of the next word very accurately uh you can use that to compress the data set so it's just a next word prediction neural network you give it some words it gives you the next word now the reason that what you get out of the training is actually quite a magical artifact is that basically the next word predition task you might think is a very simple objective but it's actually a pretty powerful objective because it forces you to learn a lot about the world inside the parameters of the neural network so here I took a random web page um at the time when I was making this talk I just grabbed it from the main page of Wikipedia and it was uh about Ruth Handler and so think about being the neural network and you're given some amount of words and trying to predict the next word in a sequence well in this case I'm highlighting here in red some of the words that would contain a lot of information and so for example in in if your objective is to predict the next word presumably your parameters have to learn a lot of this knowledge you have to know about Ruth and Handler and when she was born and when she died uh who she was uh what she's done and so on and so in the task of next word prediction you're learning a ton about the world and all this knowledge is being compressed into the weights uh the parameters now how do we actually use these neural networks well once we've trained them I showed you that the model inference um is a very simple process we basically generate uh what comes next we sample from the model so we pick a word um and then we continue feeding it back in and get the next word and continue feeding that back in so we can iterate this process and this network then dreams internet documents so for example if we just run the neural network or as we say perform inference uh we would get sort of like web page dreams you can almost think about it that way right because this network was trained on web pages and then you can sort of like Let it Loose so on the left we have some kind of a Java code dream it looks like in the middle we have some kind of a what looks like almost like an Amazon product dream um and on the right we have something that almost looks like Wikipedia article focusing for a bit on the middle one as an example the title the author the ISBN number everything else this is all just totally made up by the network uh the network is dreaming text uh from the distribution that it\n"
     ]
    }
   ],
   "source": [
    "# original text\n",
    "sample_text = df_srt_v2['text'][0]\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow, this explanation of how AI neural networks learn and generate text is fascinating! Let's break down the key points:\n",
      "\n",
      "**The Core Idea:**\n",
      "\n",
      "* **Predicting \"the next word\":** At its heart, we are training a powerful language model to predict the next word in a sequence.  It learns to do this by analyzing vast amounts of text data (like those Wikipedia pages!). \n",
      "* **Compression Magic:** This prediction process forces the neural network to learn a deep understanding of language structures and relationships between words. It does so by effectively \"compressing\" the information contained in millions (billions) of web pages into simpler units.  \n",
      "\n",
      "**Why it's Powerful:**\n",
      "\n",
      "* **Next-Word Prediction is Deep Learning:** Think about it like this:  by learning to predict “the next word”, the network acquires an amazing level of language comprehension that mirrors what we learn by reading and experiencing the world through written communication.\n",
      "* **Knowledge, In Essence:** The process forces the network to understand everything from a person's biography (who Ruth Handler was, her life story) to more nuanced concepts about the history of toys and design. \n",
      "\n",
      "**Using the Networks**\n",
      "\n",
      "* **Simple text generation:** Once trained, we just feed in some starting words (\"C set on a... \") and the model predicts the rest of the text sentence by sentence!  It's like a digital writer that can create endless prose!\n",
      "* **Dream-like Text Generation: ** Think about it: if you just ask the network to \"imagine a Wikipedia article about Amazon,\" it might generate a text draft. The key point is, this system captures language through prediction and then learns with its training data. \n",
      "\n",
      "**Beyond Language:** These networks aren't confined to language - they can be trained on other types of information too! As long as the structure/content is represented by something like numbers or categorical variables, AI models will learn from it effectively. \n",
      "\n",
      "\n",
      " **The Takeaways:**\n",
      "\n",
      "* This \"training process\" results in remarkably adept text generation that mirrors natural language understanding at its core.\n",
      "* The simplicity of this neural network allows for rapid development, and its performance surpasses earlier AI language models, making it relevant for numerous tasks (writing articles, storytelling, generating different kinds of content). \n",
      "\n",
      "\n",
      "Let me know if you'd like to explore specific aspects of the model in greater detail!  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summarised text\n",
    "sample_smry = generate_smry(sample_text)\n",
    "print(sample_smry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_smry_file(df_in, text_col='text'):\n",
    "    '''\n",
    "    Iterates through each row and generates summary column content for the text in 'text_col'.\n",
    "    '''\n",
    "    df = df_in.copy()\n",
    "    print(f\"INFO: initiated summary generation\")\n",
    "    print(f\"INFO: total text blocks {df.shape[0]}\")\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Generating Summaries\"):\n",
    "        smry_text = generate_smry(row[text_col])\n",
    "        df.loc[index, 'smry_text'] = smry_text\n",
    "    \n",
    "    print(f\"INFO: summary generation finished\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: initiated summary generation\n",
      "INFO: total text blocks 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3be0b61fd9e4bd680de91d2df9c4ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Summaries:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: summary generation finished\n"
     ]
    }
   ],
   "source": [
    "df_blocksmry = generate_smry_file(df_srt_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a breakdown of how AI model assistants, like ChatGPT, are created.\n",
      "\n",
      "**1. Pre-training: Building the Foundation (Large Data Set)**\n",
      "\n",
      "* **Data Collection:** A massive amount of text data is scraped from the internet—books, articles, websites, social media posts.  Think hundreds of terabytes!\n",
      "* **Goal:** This initial training stage involves teaching the model to understand the vast array of human language patterns, grammar, common expressions, and even subtle nuances within the raw data. Think of it as getting your AI child to read a ton of books to build its basic literacy skills.\n",
      "* **Challenge:** Finding high-quality training data is crucial.  While the internet boasts a wealth of information, much of it requires filtering and cleaning for quality and reliability.\n",
      "\n",
      "**2. Fine-tuning: Adapting to User Interaction (High-Quality Q&A Set)**\n",
      "\n",
      "* **Data Collection:** Carefully curated question-and-answer dataset focusing on conversational styles. \n",
      "    * Example: \"Can you help me with this code? It seems there's a bug.\" \n",
      "* **Goal:** This stage refines the model's abilities to specifically follow a dialogue format. Each answer needs to be a helpful and relevant response to user prompts.  Think of it as learning how to give precise, coherent answers that suit casual conversation. \n",
      "* **Human Input:** Humans play a vital role defining specific ways the AI assistant should respond, ensuring the overall consistency is maintained.  \n",
      "\n",
      "**Key Elements in Fine-Tuning**\n",
      "\n",
      "* **Alignment: ** This is the crucial aspect that differentiates fine-tuning from pre-training.  In this stage, we go beyond just patterns and teach the model how to be conversational (like teaching language skills). It's about adjusting its style and approach for specific tasks or conversational prompts. \n",
      "    * Think of it as teaching your chatbot how to become a fluent and helpful conversational AI!\n",
      "* **Labeling Instructions:** These detailed guidelines (written and annotated by experts, like human trainers) show the model how answers should be structured based on pre-defined topics, rules, and styles\n",
      "\n",
      "**3. The Result:  A More Conversational & Useful AI Assistant**\n",
      "\n",
      "Fine-tuning transforms the model from a general language learner to a chatbot that understands conversational cues (context, tone) and can provide user-requested information in a more natural and helpful way.\n",
      "\n",
      "**The Role of Companies in Fine-Tuning:**\n",
      "\n",
      "\n",
      "* **Companies like OpenAI (ChatGPT), Google**, and **Anthropic**, etc., actively collect and label questions to train these assistant models, employing human specialists for guidance on the quality of responses and consistency.  They need to create a vast collection of high-quality question-and-answer pairs that cover a wide range of topics.\n",
      "\n",
      "\n",
      "**In Summary:** The key lies in striking a balance between acquiring knowledge from the world and training it towards specific conversational skills which is what fine-tuning effectively does to produce an AI assistant model ready for a range of user interaction. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check generated summary\n",
    "print(df_blocksmry.loc[3, 'smry_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Block</th>\n",
       "      <th>text</th>\n",
       "      <th>start_time</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>smry_text</th>\n",
       "      <th>smry_text_length</th>\n",
       "      <th>smry_word_count</th>\n",
       "      <th>uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hi everyone so recently I gave a 30-minute ta...</td>\n",
       "      <td>0.160</td>\n",
       "      <td>10650</td>\n",
       "      <td>2029</td>\n",
       "      <td>Wow, this explanation of how AI neural network...</td>\n",
       "      <td>2395</td>\n",
       "      <td>363</td>\n",
       "      <td>zjkBMFhNj_g__B1__S0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>for example write a poem about the company sc...</td>\n",
       "      <td>180.159</td>\n",
       "      <td>10970</td>\n",
       "      <td>2085</td>\n",
       "      <td>## Summary of Neural Network Basics, a Technic...</td>\n",
       "      <td>3138</td>\n",
       "      <td>447</td>\n",
       "      <td>zjkBMFhNj_g__B2__S180.159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>and so it's kind of like a lossy compression ...</td>\n",
       "      <td>361.479</td>\n",
       "      <td>11064</td>\n",
       "      <td>2067</td>\n",
       "      <td>Great explanation! You've eloquently delved in...</td>\n",
       "      <td>2883</td>\n",
       "      <td>414</td>\n",
       "      <td>zjkBMFhNj_g__B3__S361.479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Block                                               text  start_time  \\\n",
       "0      1   hi everyone so recently I gave a 30-minute ta...       0.160   \n",
       "1      2   for example write a poem about the company sc...     180.159   \n",
       "2      3   and so it's kind of like a lossy compression ...     361.479   \n",
       "\n",
       "   text_length  word_count                                          smry_text  \\\n",
       "0        10650        2029  Wow, this explanation of how AI neural network...   \n",
       "1        10970        2085  ## Summary of Neural Network Basics, a Technic...   \n",
       "2        11064        2067  Great explanation! You've eloquently delved in...   \n",
       "\n",
       "   smry_text_length  smry_word_count                        uid  \n",
       "0              2395              363     zjkBMFhNj_g__B1__S0.16  \n",
       "1              3138              447  zjkBMFhNj_g__B2__S180.159  \n",
       "2              2883              414  zjkBMFhNj_g__B3__S361.479  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blocksmry['smry_text_length'] = df_blocksmry['smry_text'].apply(len)\n",
    "df_blocksmry['smry_word_count'] = df_blocksmry['smry_text'].apply(lambda x : len(x.split()))\n",
    "df_blocksmry['uid'] = df_blocksmry.apply(lambda x: video_id + '__B' + str(x['Block']) + '__S' + str(x['start_time']), axis=1)\n",
    "\n",
    "df_blocksmry.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 3))\n",
    "\n",
    "df_blocksmry['word_count'].plot.hist(bins=12, alpha=0.8, ax=axes[0], title='Word Count')\n",
    "df_blocksmry['smry_word_count'].plot.hist(bins=12, alpha=0.8, ax=axes[1], title='Summary Word Count')\n",
    "df_blocksmry['smry_text_length'].plot.hist(bins=12, alpha=0.8, ax=axes[2], title='Summary Text Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(filename):\n",
    "    '''\n",
    "    filename : filepath and filename(.pkl file) as a single string\n",
    "    This function can be used to read any pickle file\n",
    "    '''\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def dump_pickle(obj, filename):\n",
    "    '''\n",
    "    obj : the object that needs to be dumped as pickle\n",
    "    filename : string path and name of the destination pickle file\n",
    "    '''\n",
    "    with open(filename, 'wb') as f_out:\n",
    "        pickle.dump(obj, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory if it doesn't exist\n",
    "os.makedirs(\"../data/summary_transcripts/\", exist_ok=True)\n",
    "\n",
    "reqd_cols = ['uid', 'text', 'smry_text']\n",
    "dump_pickle(df_blocksmry[reqd_cols], \"../data/summary_transcripts/smry_vid_\"+video_id+\".pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Clean Text (unsuccessful attempt | not included in the project scripts )\n",
    "This is just supposed to be a cleaned transcript, not a summary. It did not work fine, otherwise this could have later been used to generate summary or could also have been used as context for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cleantxt(transcript, llm_model='gemma2:2b'):\n",
    "    '''\n",
    "    This function takes in a 'transcript' text and generates summarized text using 'llm_model' specified.\n",
    "    '''\n",
    "    cleantxt_prompt = \"\"\"As a professional editor, your task is to clean the provided YouTube transcript without omitting any information from the original text.\n",
    "        The generated clean output should be grammatically correct, have no spelling mistakes and have clarity.\n",
    "        It should not have filler words such as 'uhm', 'mhm', and similar phrases that can be heard in audio but does not make any sense in written text.\n",
    "        Retain all the original phrases for authenticity.\n",
    "        Don't add any new information, don't express your opinions about the speaker and don't suggest any follow up query.\n",
    "        Do not praise the speaker or me. Just provide the cleaned text as per above directions.\n",
    "        \n",
    "        TRANSCRIPT: {INPUT_TRANSCRIPT}\"\"\"\n",
    "\n",
    "    prompt = cleantxt_prompt.format(INPUT_TRANSCRIPT = transcript)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        # temperature=0,    # remove randomness for deterministic output but not using it as it makes summary clumsy with phrases like 'you stated correctly...', 'you explained it well...' etc.\n",
    "        seed=72\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2029  hi everyone so recently I gave a 30-minute talk on large language models just kind of like an intro talk um unfortunately that talk was not recorded but a lot of people came to me after the talk and they told me that uh they really liked the talk so I would just I thought I would just re-record it and basically put it up on YouTube so here we go the busy person's intro to large language models director Scott okay so let's begin first of all what is a large language model really well a large language model is just two files right um there will be two files in this hypothetical directory so for example working with a specific example of the Llama 270b model this is a large language model released by meta Ai and this is basically the Llama series of language models the second iteration of it and this is the 70 billion parameter model of uh of this series so there's multiple models uh belonging to the Llama 2 Series uh 7 billion um 13 billion 34 billion and 70 billion is the biggest one now many people like this model specifically because it is probably today the most powerful open weights model so basically the weights and the architecture and a paper was all released by meta so anyone can work with this model very easily uh by themselves uh this is unlike many other language models that you might be familiar with for example if you're using chat GPT or something like that uh the model architecture was never released it is owned by open aai and you're allowed to use the language model through a web interface but you don't have actually access to that model so in this case the Llama 270b model is really just two files on your file system the parameters file and the Run uh some kind of a code that runs those parameters so the parameters are basically the weights or the parameters of this neural network that is the language model we'll go into that in a bit because this is a 70 billion parameter model uh every one of those parameters is stored as 2 bytes and so therefore the parameters file here is 140 gigabytes and it's two bytes because this is a float 16 uh number as the data type now in addition to these parameters that's just like a large list of parameters uh for that neural network you also need something that runs that neural network and this piece of code is implemented in our run file now this could be a C file or a python file or any other programming language really uh it can be written any arbitrary language but C is sort of like a very simple language just to give you a sense and uh it would only require about 500 lines of C with no other dependencies to implement the the uh neural network architecture uh and that uses basically the parameters to run the model so it's only these two files you can take these two files and you can take your MacBook and this is a fully self-contained package this is everything that's necessary you don't need any connectivity to the internet or anything else you can take these two files you compile your C code you get a binary that you can point at the parameters and you can talk to this language model so for example you can send it text like for example write a poem about the company scale Ai and this language model will start generating text and in this case it will follow the directions and give you a poem about scale AI now the reason that I'm picking on scale AI here and you're going to see that throughout the talk is because the event that I originally presented uh this talk with was run by scale Ai and so I'm picking on them throughout uh throughout the slides a little bit just in an effort to make it concrete so this is how we can run the model just requires two files just requires a MacBook I'm slightly cheating here because this was not actually in terms of the speed of this uh video here this was not running a 70 billion parameter model it was only running a 7 billion parameter Model A 70b would be running about 10 times slower but I wanted to give you an idea of uh sort of just the text generation and what that looks like so not a lot is necessary to run the model this is a very small package but the computational complexity really comes in when we'd like to get those parameters so how do we get the parameters and where are they from uh because whatever is in the run. C file um the neural network architecture and sort of the forward pass of that Network everything is algorithmically understood and open and and so on but the magic really is in the parameters and how do we obtain them so to obtain the parameters um basically the model training as we call it is a lot more involved than model inference which is the part that I showed you earlier so model inference is just running it on your MacBook model training is a competition very involved process process so basically what we're doing can best be sort of understood as kind of a compression of a good chunk of Internet so because llama 270b is an open source model we know quite a bit about how it was trained because meta released that information in paper so these are some of the numbers of what's involved you basically take a chunk of the internet that is roughly you should be thinking 10 terab of text this typically comes from like a crawl of the internet so just imagine uh just collecting tons of text from all kinds of different websites and collecting it together so you take a large cheun of internet then you procure a GPU cluster um and uh these are very specialized computers intended for very heavy computational workloads like training of neural networks you need about 6,000 gpus and you would run this for about 12 days uh to get a llama 270b and this would cost you about $2 million and what this is doing is basically it is compressing this uh large chunk of text into what you can think of as a kind of a zip file so these parameters that I showed you in an earlier slide are best kind of thought of as like a zip file of the internet and in this case what would come out are these parameters 140 GB so you can see that the compression ratio here is roughly like 100x uh roughly speaking but this is not exactly a zip file because a zip file is lossless compression What's Happening Here is a lossy compression we're just kind of like getting a kind of a Gestalt of the text that we trained on we don't have an identical copy of it in these parameters and so it's kind of like a lossy compression you can think about it that way the one more thing to point out here is these numbers here are actually by today's standards in terms of state-of-the-art rookie numbers uh so if you want to think about state-of-the-art neural networks like say what you might use in chpt or Claude or Bard or something like that uh these numbers are off by factor of 10 or more so you would just go in then you just like start multiplying um by quite a bit more and that's why these training runs today are many tens or even potentially hundreds of millions of dollars very large clusters very large data sets and this process here is very involved to get those parameters once you have those parameters running the neural network is fairly computationally cheap okay so what is this neural network really doing right I mentioned that there are these parameters um this neural network basically is just trying to predict the next word in a sequence you can think about it that way so you can feed in a sequence of words for example C set on a this feeds into a neural net and these parameters are dispersed throughout this neural network and there's neurons and they're connected to each other and they all fire in a certain way you can think about it that way um and out comes a prediction for what word comes next so for example in this case this neural network might predict that in this context of for Words the next word will probably be a Matt with say 97% probability so this is fundamentally the problem that the neural network is performing and this you can show mathematically that there's a very close relationship between prediction and compression which is why I sort of allude to this neural network as a kind of training it is kind of like a compression of the internet um because if you can predict uh sort of the next word very accurately uh you can use that to compress the data set so it's just a next word prediction neural network you give it some words it gives you the next word now the reason that what you get out of the training is actually quite a magical artifact is that basically the next word predition task you might think is a very simple objective but it's actually a pretty powerful objective because it forces you to learn a lot about the world inside the parameters of the neural network so here I took a random web page um at the time when I was making this talk I just grabbed it from the main page of Wikipedia and it was uh about Ruth Handler and so think about being the neural network and you're given some amount of words and trying to predict the next word in a sequence well in this case I'm highlighting here in red some of the words that would contain a lot of information and so for example in in if your objective is to predict the next word presumably your parameters have to learn a lot of this knowledge you have to know about Ruth and Handler and when she was born and when she died uh who she was uh what she's done and so on and so in the task of next word prediction you're learning a ton about the world and all this knowledge is being compressed into the weights uh the parameters now how do we actually use these neural networks well once we've trained them I showed you that the model inference um is a very simple process we basically generate uh what comes next we sample from the model so we pick a word um and then we continue feeding it back in and get the next word and continue feeding that back in so we can iterate this process and this network then dreams internet documents so for example if we just run the neural network or as we say perform inference uh we would get sort of like web page dreams you can almost think about it that way right because this network was trained on web pages and then you can sort of like Let it Loose so on the left we have some kind of a Java code dream it looks like in the middle we have some kind of a what looks like almost like an Amazon product dream um and on the right we have something that almost looks like Wikipedia article focusing for a bit on the middle one as an example the title the author the ISBN number everything else this is all just totally made up by the network uh the network is dreaming text uh from the distribution that it\n"
     ]
    }
   ],
   "source": [
    "# original text\n",
    "sample_text = df_srt_v2['text'][0]\n",
    "print(len(sample_text.split()), sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363 Wow, this explanation of how AI neural networks learn and generate text is fascinating! \n",
      "\n",
      "Here's a breakdown of the key points:\n",
      "\n",
      "**The Basics:**\n",
      "\n",
      "* **\"Lossy compression\":** Instead of perfectly recreating every detail of an original document (like a zip file where you lose information to make it smaller), this network learns patterns and general trends, resulting in highly compressed representations. Think of learning about Ruth Handler from just reading her Wikipedia page for a few minutes. \n",
      "* **Word prediction:** This is the core of their training - predicting the next word in a sequence, similar how predictive text works on your phone. However, this is much deeper than simple predictions.\n",
      "* **\"A Gestalt of the internet\":** The parameters form a snapshot of the \"internet knowledge,\" capturing patterns learned from text and websites.  The resulting \"dreams\" of random content showcase what AI is capable of learning and representing. \n",
      "\n",
      "**How it Works (Simplified)** \n",
      "\n",
      "1. **Training:** A huge dataset like Wikipedia pages is fed into the network over time. \n",
      "2. **Prediction:** Given a starting word, the network predicts the next word based on its learned patterns – the network tries to make 'predictions' about what's next in the sequence of words it sees. \n",
      "3. **Inference:** The network 'dreams up' further text by choosing from what it has learned.  It can generate different dreams - think of a blog post, or even product lists and similar outputs. \n",
      "\n",
      "**Importance and Applications:**\n",
      "\n",
      "* **State-of-the-art models:** This model excels at language prediction because it learns complex relationship within large sets of data.  Models are often 10x larger in parameters than similar models you might typically encounter.\n",
      "* **Applications:** From chatbots to personalized writing tools, creative text generation to scientific text mining, these AI neural networks are changing fields!\n",
      "\n",
      "**Further points to consider:**  \n",
      "- **The role of training data:** The quality and quantity of data significantly influences learning and the model's final output. A good dataset leads to better dream production.\n",
      "- **Ethical considerations:** Concerns about bias or misinformation when models are trained on biased datasets can be discussed alongside these advancements in language models.\n",
      "\n",
      "**Overall, this explanation provides a great window into the power and future of AI language modeling!** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cleaned text\n",
    "sample_smry = generate_cleantxt(sample_text)\n",
    "print(len(sample_smry.split()), sample_smry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and Indexing using ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps\n",
    "# - [done] Read pickle & replace with json read and dump functions\n",
    "# - [done] Install sentence embedding library\n",
    "# - Index text data first\n",
    "# - Test text retrieval\n",
    "# - Generate vector data\n",
    "# - Save vector embeddings somewhere\n",
    "# - Test vector retrieval\n",
    "# - Test hybrid retrieval\n",
    "# - next: work on scripts and gold standard data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>smry_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zjkBMFhNj_g__B1__S0.16</td>\n",
       "      <td>hi everyone so recently I gave a 30-minute ta...</td>\n",
       "      <td>Wow, this explanation of how AI neural network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zjkBMFhNj_g__B2__S180.159</td>\n",
       "      <td>for example write a poem about the company sc...</td>\n",
       "      <td>## Summary of Neural Network Basics, a Technic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         uid  \\\n",
       "0     zjkBMFhNj_g__B1__S0.16   \n",
       "1  zjkBMFhNj_g__B2__S180.159   \n",
       "\n",
       "                                                text  \\\n",
       "0   hi everyone so recently I gave a 30-minute ta...   \n",
       "1   for example write a poem about the company sc...   \n",
       "\n",
       "                                           smry_text  \n",
       "0  Wow, this explanation of how AI neural network...  \n",
       "1  ## Summary of Neural Network Basics, a Technic...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blocksmry = read_pickle(\"../data/summary_transcripts/smry_vid_\"+video_id+\".pkl\")\n",
    "print(df_blocksmry.shape)\n",
    "df_blocksmry.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df_to_json(df, filename):\n",
    "    df_json = df.to_dict(orient=\"records\")\n",
    "    with open(filename, 'wt') as f_out:\n",
    "        json.dump(df_json, f_out, indent=2)\n",
    "\n",
    "export_df_to_json(df_blocksmry, \"../data/summary_transcripts/smry_vid_\"+video_id+\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,\n",
       " {'uid': 'zjkBMFhNj_g__B1__S0.16',\n",
       "  'text': \" hi everyone so recently I gave a 30-minute talk on large language models just kind of like an intro talk um unfortunately that talk was not recorded but a lot of people came to me after the talk and they told me that uh they really liked the talk so I would just I thought I would just re-record it and basically put it up on YouTube so here we go the busy person's intro to large language models director Scott okay so let's begin first of all what is a large language model really well a large language model is just two files right um there will be two files in this hypothetical directory so for example working with a specific example of the Llama 270b model this is a large language model released by meta Ai and this is basically the Llama series of language models the second iteration of it and this is the 70 billion parameter model of uh of this series so there's multiple models uh belonging to the Llama 2 Series uh 7 billion um 13 billion 34 billion and 70 billion is the biggest one now many people like this model specifically because it is probably today the most powerful open weights model so basically the weights and the architecture and a paper was all released by meta so anyone can work with this model very easily uh by themselves uh this is unlike many other language models that you might be familiar with for example if you're using chat GPT or something like that uh the model architecture was never released it is owned by open aai and you're allowed to use the language model through a web interface but you don't have actually access to that model so in this case the Llama 270b model is really just two files on your file system the parameters file and the Run uh some kind of a code that runs those parameters so the parameters are basically the weights or the parameters of this neural network that is the language model we'll go into that in a bit because this is a 70 billion parameter model uh every one of those parameters is stored as 2 bytes and so therefore the parameters file here is 140 gigabytes and it's two bytes because this is a float 16 uh number as the data type now in addition to these parameters that's just like a large list of parameters uh for that neural network you also need something that runs that neural network and this piece of code is implemented in our run file now this could be a C file or a python file or any other programming language really uh it can be written any arbitrary language but C is sort of like a very simple language just to give you a sense and uh it would only require about 500 lines of C with no other dependencies to implement the the uh neural network architecture uh and that uses basically the parameters to run the model so it's only these two files you can take these two files and you can take your MacBook and this is a fully self-contained package this is everything that's necessary you don't need any connectivity to the internet or anything else you can take these two files you compile your C code you get a binary that you can point at the parameters and you can talk to this language model so for example you can send it text like for example write a poem about the company scale Ai and this language model will start generating text and in this case it will follow the directions and give you a poem about scale AI now the reason that I'm picking on scale AI here and you're going to see that throughout the talk is because the event that I originally presented uh this talk with was run by scale Ai and so I'm picking on them throughout uh throughout the slides a little bit just in an effort to make it concrete so this is how we can run the model just requires two files just requires a MacBook I'm slightly cheating here because this was not actually in terms of the speed of this uh video here this was not running a 70 billion parameter model it was only running a 7 billion parameter Model A 70b would be running about 10 times slower but I wanted to give you an idea of uh sort of just the text generation and what that looks like so not a lot is necessary to run the model this is a very small package but the computational complexity really comes in when we'd like to get those parameters so how do we get the parameters and where are they from uh because whatever is in the run. C file um the neural network architecture and sort of the forward pass of that Network everything is algorithmically understood and open and and so on but the magic really is in the parameters and how do we obtain them so to obtain the parameters um basically the model training as we call it is a lot more involved than model inference which is the part that I showed you earlier so model inference is just running it on your MacBook model training is a competition very involved process process so basically what we're doing can best be sort of understood as kind of a compression of a good chunk of Internet so because llama 270b is an open source model we know quite a bit about how it was trained because meta released that information in paper so these are some of the numbers of what's involved you basically take a chunk of the internet that is roughly you should be thinking 10 terab of text this typically comes from like a crawl of the internet so just imagine uh just collecting tons of text from all kinds of different websites and collecting it together so you take a large cheun of internet then you procure a GPU cluster um and uh these are very specialized computers intended for very heavy computational workloads like training of neural networks you need about 6,000 gpus and you would run this for about 12 days uh to get a llama 270b and this would cost you about $2 million and what this is doing is basically it is compressing this uh large chunk of text into what you can think of as a kind of a zip file so these parameters that I showed you in an earlier slide are best kind of thought of as like a zip file of the internet and in this case what would come out are these parameters 140 GB so you can see that the compression ratio here is roughly like 100x uh roughly speaking but this is not exactly a zip file because a zip file is lossless compression What's Happening Here is a lossy compression we're just kind of like getting a kind of a Gestalt of the text that we trained on we don't have an identical copy of it in these parameters and so it's kind of like a lossy compression you can think about it that way the one more thing to point out here is these numbers here are actually by today's standards in terms of state-of-the-art rookie numbers uh so if you want to think about state-of-the-art neural networks like say what you might use in chpt or Claude or Bard or something like that uh these numbers are off by factor of 10 or more so you would just go in then you just like start multiplying um by quite a bit more and that's why these training runs today are many tens or even potentially hundreds of millions of dollars very large clusters very large data sets and this process here is very involved to get those parameters once you have those parameters running the neural network is fairly computationally cheap okay so what is this neural network really doing right I mentioned that there are these parameters um this neural network basically is just trying to predict the next word in a sequence you can think about it that way so you can feed in a sequence of words for example C set on a this feeds into a neural net and these parameters are dispersed throughout this neural network and there's neurons and they're connected to each other and they all fire in a certain way you can think about it that way um and out comes a prediction for what word comes next so for example in this case this neural network might predict that in this context of for Words the next word will probably be a Matt with say 97% probability so this is fundamentally the problem that the neural network is performing and this you can show mathematically that there's a very close relationship between prediction and compression which is why I sort of allude to this neural network as a kind of training it is kind of like a compression of the internet um because if you can predict uh sort of the next word very accurately uh you can use that to compress the data set so it's just a next word prediction neural network you give it some words it gives you the next word now the reason that what you get out of the training is actually quite a magical artifact is that basically the next word predition task you might think is a very simple objective but it's actually a pretty powerful objective because it forces you to learn a lot about the world inside the parameters of the neural network so here I took a random web page um at the time when I was making this talk I just grabbed it from the main page of Wikipedia and it was uh about Ruth Handler and so think about being the neural network and you're given some amount of words and trying to predict the next word in a sequence well in this case I'm highlighting here in red some of the words that would contain a lot of information and so for example in in if your objective is to predict the next word presumably your parameters have to learn a lot of this knowledge you have to know about Ruth and Handler and when she was born and when she died uh who she was uh what she's done and so on and so in the task of next word prediction you're learning a ton about the world and all this knowledge is being compressed into the weights uh the parameters now how do we actually use these neural networks well once we've trained them I showed you that the model inference um is a very simple process we basically generate uh what comes next we sample from the model so we pick a word um and then we continue feeding it back in and get the next word and continue feeding that back in so we can iterate this process and this network then dreams internet documents so for example if we just run the neural network or as we say perform inference uh we would get sort of like web page dreams you can almost think about it that way right because this network was trained on web pages and then you can sort of like Let it Loose so on the left we have some kind of a Java code dream it looks like in the middle we have some kind of a what looks like almost like an Amazon product dream um and on the right we have something that almost looks like Wikipedia article focusing for a bit on the middle one as an example the title the author the ISBN number everything else this is all just totally made up by the network uh the network is dreaming text uh from the distribution that it\",\n",
       "  'smry_text': 'Wow, this explanation of how AI neural networks learn and generate text is fascinating! \\n\\nHere\\'s a breakdown of the key points:\\n\\n**The Basics:**\\n\\n* **\"Lossy compression\":** Instead of perfectly recreating every detail of an original document (like a zip file where you lose information to make it smaller), this network learns patterns and general trends, resulting in highly compressed representations. Think of learning about Ruth Handler from just reading her Wikipedia page for a few minutes. \\n* **Word prediction:** This is the core of their training - predicting the next word in a sequence, similar how predictive text works on your phone. However, this is much deeper than simple predictions.\\n* **\"A Gestalt of the internet\":** The parameters form a snapshot of the \"internet knowledge,\" capturing patterns learned from text and websites.  The resulting \"dreams\" of random content showcase what AI is capable of learning and representing. \\n\\n**How it Works (Simplified)** \\n\\n1. **Training:** A huge dataset like Wikipedia pages is fed into the network over time. \\n2. **Prediction:** Given a starting word, the network predicts the next word based on its learned patterns – the network tries to make \\'predictions\\' about what\\'s next in the sequence of words it sees. \\n3. **Inference:** The network \\'dreams up\\' further text by choosing from what it has learned.  It can generate different dreams - think of a blog post, or even product lists and similar outputs. \\n\\n**Importance and Applications:**\\n\\n* **State-of-the-art models:** This model excels at language prediction because it learns complex relationship within large sets of data.  Models are often 10x larger in parameters than similar models you might typically encounter.\\n* **Applications:** From chatbots to personalized writing tools, creative text generation to scientific text mining, these AI neural networks are changing fields!\\n\\n**Further points to consider:**  \\n- **The role of training data:** The quality and quantity of data significantly influences learning and the model\\'s final output. A good dataset leads to better dream production.\\n- **Ethical considerations:** Concerns about bias or misinformation when models are trained on biased datasets can be discussed alongside these advancements in language models.\\n\\n**Overall, this explanation provides a great window into the power and future of AI language modeling!** \\n'})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_jsonfile(filename):\n",
    "    with open(filename, 'rt') as f:\n",
    "        data_json = json.load(f)\n",
    "    return data_json\n",
    "\n",
    "documents = load_jsonfile(\"../data/summary_transcripts/smry_vid_\"+video_id+\".json\")\n",
    "len(documents), documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': '88914ba41ef1', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'yK9hM3oYTMSKypdNcXrWPg', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200')\n",
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'video-transcripts'})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"uid\": {\"type\": \"text\"},\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"smry_text\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"video-transcripts\"\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3cd086965c41009be547d558ad3da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search_txt(query, index_name, n_results=5):\n",
    "    '''\n",
    "    This function performs text based ES search for the provided 'query' in the index 'index_name' and returns the most relevant 'n_results' results.\n",
    "    '''\n",
    "    search_query = {\n",
    "        \"size\": n_results,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"smry_text\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'uid': 'zjkBMFhNj_g__B3__S361.479',\n",
       "  'text': \" and so it's kind of like a lossy compression you can think about it that way the one more thing to point out here is these numbers here are actually by today's standards in terms of state-of-the-art rookie numbers uh so if you want to think about state-of-the-art neural networks like say what you might use in chpt or Claude or Bard or something like that uh these numbers are off by factor of 10 or more so you would just go in then you just like start multiplying um by quite a bit more and that's why these training runs today are many tens or even potentially hundreds of millions of dollars very large clusters very large data sets and this process here is very involved to get those parameters once you have those parameters running the neural network is fairly computationally cheap okay so what is this neural network really doing right I mentioned that there are these parameters um this neural network basically is just trying to predict the next word in a sequence you can think about it that way so you can feed in a sequence of words for example C set on a this feeds into a neural net and these parameters are dispersed throughout this neural network and there's neurons and they're connected to each other and they all fire in a certain way you can think about it that way um and out comes a prediction for what word comes next so for example in this case this neural network might predict that in this context of for Words the next word will probably be a Matt with say 97% probability so this is fundamentally the problem that the neural network is performing and this you can show mathematically that there's a very close relationship between prediction and compression which is why I sort of allude to this neural network as a kind of training it is kind of like a compression of the internet um because if you can predict uh sort of the next word very accurately uh you can use that to compress the data set so it's just a next word prediction neural network you give it some words it gives you the next word now the reason that what you get out of the training is actually quite a magical artifact is that basically the next word predition task you might think is a very simple objective but it's actually a pretty powerful objective because it forces you to learn a lot about the world inside the parameters of the neural network so here I took a random web page um at the time when I was making this talk I just grabbed it from the main page of Wikipedia and it was uh about Ruth Handler and so think about being the neural network and you're given some amount of words and trying to predict the next word in a sequence well in this case I'm highlighting here in red some of the words that would contain a lot of information and so for example in in if your objective is to predict the next word presumably your parameters have to learn a lot of this knowledge you have to know about Ruth and Handler and when she was born and when she died uh who she was uh what she's done and so on and so in the task of next word prediction you're learning a ton about the world and all this knowledge is being compressed into the weights uh the parameters now how do we actually use these neural networks well once we've trained them I showed you that the model inference um is a very simple process we basically generate uh what comes next we sample from the model so we pick a word um and then we continue feeding it back in and get the next word and continue feeding that back in so we can iterate this process and this network then dreams internet documents so for example if we just run the neural network or as we say perform inference uh we would get sort of like web page dreams you can almost think about it that way right because this network was trained on web pages and then you can sort of like Let it Loose so on the left we have some kind of a Java code dream it looks like in the middle we have some kind of a what looks like almost like an Amazon product dream um and on the right we have something that almost looks like Wikipedia article focusing for a bit on the middle one as an example the title the author the ISBN number everything else this is all just totally made up by the network uh the network is dreaming text uh from the distribution that it was trained on it's it's just mimicking these documents but this is all kind of like hallucinated so for example the ISBN number this number probably I would guess almost certainly does not exist uh the model Network just knows that what comes after ISB and colon is some kind of a number of roughly this length and it's got all these digits and it just like puts it in it just kind of like puts in whatever looks reasonable so it's parting the training data set Distribution on the right the black nose days I looked at up and it is actually a kind of fish um and what's Happening Here is this text verbatim is not found in a training set documents but this information if you actually look it up is actually roughly correct with respect to this fish and so the network has knowledge about this fish it knows a lot about this fish it's not going to exactly parrot the documents that it saw in the training set but again it's some kind of a l some kind of a lossy compression of the internet it kind of remembers the gal it kind of knows the knowledge and it just kind of like goes and it creates the form it creates kind of like the correct form and fills it with some of its knowledge and you're never 100% sure if what it comes up with is as we call hallucination or like an incorrect answer or like a correct answer necessarily so some of the stuff could be memorized and some of it is not memorized and you don't exactly know which is which um but for the most part this is just kind of like hallucinating or like dreaming internet text from its data distribution okay let's now switch gears to how does this network work how does it actually perform this next word prediction task what goes on inside it well this is where things complicate a little bit this is kind of like the schematic diagram of the neural network um if we kind of like zoom in into the toy diagram of this neural net this is what we call the Transformer neural network architecture and this is kind of like a diagram of it now what's remarkable about these neural nuts is we actually understand uh in full detail the architecture we know exactly what mathematical operations happen at all the different stages of it uh the problem is that these 100 billion parameters are dispersed throughout the entire neural network work and so basically these buildon parameters uh of billions of parameters are throughout the neural nut and all we know is how to adjust these parameters iteratively to make the network as a whole better at the next word prediction task so we know how to optimize these parameters we know how to adjust them over time to get a better next word prediction but we don't actually really know what these 100 billion parameters are doing we can measure that it's getting better at the next word prediction but we don't know how these parameters collaborate to actually perform that um we have some kind of models that you can try to think through on a high level for what the network might be doing so we kind of understand that they build and maintain some kind of a knowledge database but even this knowledge database is very strange and imperfect and weird uh so a recent viral example is what we call the reversal course uh so as an example if you go to chat GPT and you talk to GPT 4 the best language model currently available you say who is Tom Cruz's mother it will tell you it's merily feifer which is correct but if you say who is merely Fifer's son it will tell you it doesn't know so this knowledge is weird and it's kind of one-dimensional and you have to sort of like this knowledge isn't just like stored and can be accessed in all the different ways you have sort of like ask it from a certain direction almost um and so that's really weird and strange and fundamentally we don't really know because all you can kind of measure is whether it works or not and with what probability so long story short think of llms as kind of like most mostly inscrutable artifacts they're not similar to anything else you might might built in an engineering discipline like they're not like a car where we sort of understand all the parts um there are these neural Nets that come from a long process of optimization and so we don't currently understand exactly how they work although there's a field called interpretability or or mechanistic interpretability trying to kind of go in and try to figure out like what all the parts of this neural net are doing and you can do that to some extent but not fully right now U but right now we kind of what treat them mostly As empirical artifacts we can give them some inputs and we can measure the outputs we can basically measure their behavior we can look at the text that they generate in many different situations and so uh I think this requires basically correspondingly sophisticated evaluations to work with these models because they're mostly empirical so now let's go to how we actually obtain an assistant so far we've only talked about these internet document generators right um and so that's the first stage of training we call that stage pre-training we're now moving to the second stage of training which we call fine-tuning and this is where we obtain what we call an assistant model because we don't actually really just want a document generators that's not very helpful for many tasks we want um to give questions to something and we want it to generate answers based on those questions so we really want an assistant model instead and the way you obtain these assistant models is fundamentally uh through the following process we basically keep the optimization identical so the training will be the same it's just the next word prediction task but we're going to s swap out the data set on which we are training so it used to be that we are trying to uh train on internet documents we're going to now swap it out for data sets that we collect manually and the way we collect them is by using lots of people so typically a company will hire people and they will give them labeling instructions and they will ask people to come up with questions and then write answers for them so here's an example of a single example um that might basically make it into your training set so there's a user and uh it says something like can you write a short introduction about the relevance of the term monopsony in economics and so on and then there's assistant and again the person fills in what the ideal response should be and the ideal response and how that is specified and what it should look like all just comes from labeling documentations that we provide these people and the engineers at a company like open or anthropic or whatever else will come up with these labeling documentations now the pre-training stage is about a\",\n",
       "  'smry_text': 'Great explanation! You\\'ve eloquently delved into the inner workings of large language models (LLMs), especially Transformer architectures.  Let\\'s break down some key points you made and their significance:\\n\\n**Challenges in Understanding LLMs:**\\n\\n* **Massive Parameters:** The sheer number of parameters in an LLM, like 100 billion, makes understanding its inner workings incredibly challenging. We can see the result (better prediction accuracy) but don\\'t yet have a clear picture of *how* that happens.  \\n* **One-Dimensional Knowledge Database:** LLMs store knowledge as something quite idiosyncratic. They lack the structured and granular insights we expect from human expertise, instead relying on patterns learned during their training. Think of it like a vast repository with unique entries, but without clearly defined categories or organizational systems that humans might use to explore information effectively.\\n* **Inability for Intuitive Inference:**  LLMs don\\'t seem to work in the ways we commonly see - they aren\\'t \"calculating\" responses like calculators. Their knowledge isn\\'t readily accessible, meaning it requires a specific prompt to get the desired output.\\n\\n**The Path to Assistants: Fine-Tuning the Landscape:**\\n* **Human Input & Labeling:** A crucial element is shifting from internet document generation (pre-training) to fine-tuning for \"Assistant\" capability by harnessing human effort. We need humans to create a structured dataset of questions aimed at different tasks,  like writing a short introduction in economics! \\n    * This gives us the data that guides the model\\'s learning, leading to better responses tailored to diverse user needs.\\n* **The Key:** LLMs become more useful and more accurate when we fine-tune them using these human interactions. They learn nuances specific to different kinds of questions and responses.\\n\\n**Open Research & Future Potential:**  Your presentation makes a strong case for a multi-pronged approach:  \\n1. **Interpretable AI**: This is the effort to understand *how* these models think; their processes (even if not fully complete) will eventually aid in more accurate evaluation, potential use cases for various fields, and even further development of efficient model training. \\n2. **Robust Evaluation Techniques**: The field of evaluation of LLMs is still in its infancy. More sophisticated measures are needed to accurately evaluate LLM performance in diverse contexts beyond simply measuring predictions.\\n\\n**Overall:** Your presentation paints a vivid picture of the mysteries surrounding today\\'s LLMs while highlighting their potential for shaping the future of human-AI collaborations. You\\'ve offered insightful observations that point towards a exciting landscape of future developments and possibilities. \\n\\n\\n\\n\\nLet me know if you would like to delve deeper into any particular aspect or have further questions! \\n'},\n",
       " {'uid': 'zjkBMFhNj_g__B6__S900.8',\n",
       "  'text': \" training so it used to be that we are trying to uh train on internet documents we're going to now swap it out for data sets that we collect manually and the way we collect them is by using lots of people so typically a company will hire people and they will give them labeling instructions and they will ask people to come up with questions and then write answers for them so here's an example of a single example um that might basically make it into your training set so there's a user and uh it says something like can you write a short introduction about the relevance of the term monopsony in economics and so on and then there's assistant and again the person fills in what the ideal response should be and the ideal response and how that is specified and what it should look like all just comes from labeling documentations that we provide these people and the engineers at a company like open or anthropic or whatever else will come up with these labeling documentations now the pre-training stage is about a large quantity of text but potentially low quality because it just comes from the internet and there's tens of or hundreds of terabyte Tech off it and it's not all very high qu uh qu quality but in this second stage uh we prefer quality over quantity so we may have many fewer documents for example 100,000 but all these documents now are conversations and they should be very high quality conversations and fundamentally people create them based on abling instructions so we swap out the data set now and we train on these Q&A documents we uh and this process is called fine tuning once you do this you obtain what we call an assistant model so this assistant model now subscribes to the form of its new training documents so for example if you give it a question like can you help me with this code it seems like there's a bug print Hello World um even though this question specifically was not part of the training Set uh the model after its fine-tuning understands that it should answer in the style of a helpful assistant to these kinds of questions and it will do that so it will sample word by word again from left to right from top to bottom all these words that are the response to this query and so it's kind of remarkable and also kind of empirical and not fully understood that these models are able to sort of like change their formatting into now being helpful assistants because they've seen so many documents of it in the fine chaining stage but they're still able to access and somehow utilize all the knowledge that was built up during the first stage the pre-training stage so roughly speaking pre-training stage is um training on trains on a ton of internet and it's about knowledge and the fine truning stage is about what we call alignment it's about uh sort of giving um it's a it's about like changing the formatting from internet documents to question and answer documents in kind of like a helpful assistant manner so roughly speaking here are the two major parts of obtaining something like chpt there's the stage one pre-training and stage two fine-tuning in the pre-training stage you get a ton of text from the internet you need a cluster of gpus so these are special purpose uh sort of uh computers for these kinds of um parel processing workloads this is not just things that you can buy and Best Buy uh these are very expensive computers and then you compress the text into this neural network into the parameters of it uh typically this could be a few uh sort of millions of dollars um and then this gives you the base model because this is a very computationally expensive part this only happens inside companies maybe once a year or once after multiple months because this is kind of like very expens very expensive to actually perform once you have the base model you enter the fing stage which is computationally a lot cheaper in this stage you write out some labeling instru instructions that basically specify how your assistant should behave then you hire people um so for example scale AI is a company that actually would um uh would work with you to actually um basically create documents according to your labeling instructions you collect 100,000 um as an example high quality ideal Q&A responses and then you would fine-tune the base model on this data this is a lot cheaper this would only potentially take like one day or something like that instead of a few uh months or something like that and you obtain what we call an assistant model then you run a lot of Valu ation you deploy this um and you monitor collect misbehaviors and for every misbehavior you want to fix it and you go to step on and repeat and the way you fix the Mis behaviors roughly speaking is you have some kind of a conversation where the Assistant gave an incorrect response so you take that and you ask a person to fill in the correct response and so the the person overwrites the response with the correct one and this is then inserted as an example into your training data and the next time you do the fine training stage uh the model will improve in that situation so that's the iterative process by which you improve this because fine tuning is a lot cheaper you can do this every week every day or so on um and companies often will iterate a lot faster on the fine training stage instead of the pre-training stage one other thing to point out is for example I mentioned the Llama 2 series The Llama 2 Series actually when it was released by meta contains contains both the base models and the assistant models so they release both of those types the base model is not directly usable because it doesn't answer questions with answers uh it will if you give it questions it will just give you more questions or it will do something like that because it's just an internet document sampler so these are not super helpful where they are helpful is that meta has done the very expensive part of these two stages they've done the stage one and they've given you the result and so you can go off and you can do your own fine-tuning uh and that gives you a ton of Freedom um but meta in addition has also released assistant models so if you just like to have a question answer uh you can use that assistant model and you can talk to it okay so those are the two major stages now see how in stage two I'm saying end or comparisons I would like to briefly double click on that because there's also a stage three of fine tuning that you can optionally go to or continue to in stage three of fine tuning you would use comparison labels uh so let me show you what this looks like the reason that we do this is that in many cases it is much easier to compare candidate answers than to write an answer yourself if you're a human labeler so consider the following concrete example suppose that the question is to write a ha cou about paper clips or something like that uh from the perspective of a labeler if I'm asked to write a ha cou that might be a very difficult task right like I might not be able to write a Hau but suppose you're given a few candidate Haus that have been generated by the assistant model from stage two well then as a labeler you could look at these Haus and actually pick the one that is much better and so in many cases it is easier to do the comparison instead of the generation and there's a stage three of fine tuning that can use these comparisons to further fine-tune the model and I'm not going to go into the full mathematical detail of this at openai this process is called reinforcement learning from Human feedback or rhf and this is kind of this optional stage three that can gain you additional performance in these language models and it utilizes these comparison labels I also wanted to show you very briefly one slide showing some of the labeling instructions that we give to humans so so this is an excerpt from the paper instruct GPT by open Ai and it just kind of shows you that we're asking people to be helpful truthful and harmless these labeling documentations though can grow to uh you know tens or hundreds of pages and can be pretty complicated um but this is roughly speaking what they look like one more thing that I wanted to mention is that I've described the process naively as humans doing all of this manual work but that's not exactly right and it's increasingly less correct and uh and that's because these language models are simultaneously getting a lot better and you can basically use human machine uh sort of collaboration to create these labels um with increasing efficiency and correctness and so for example you can get these language models to sample answers and then people sort of like cherry-pick parts of answers to create one sort of single best answer or you can ask these models to try to check your work or you can try to uh ask them to create comparisons and then you're just kind of like in an oversight role over it so this is kind of a slider that you can determine and increasingly these models are getting better uh wor moving the slider sort of to the right okay finally I wanted to show you a leaderboard of the current leading larger language models out there so this for example is a chatbot Arena it is managed by team at Berkeley and what they do here is they rank the different language models by their ELO rating and the way you calculate ELO is very similar to how you would calculate it in chess so different chess players play each other and uh you depending on the win rates against each other you can calculate the their ELO scores you can do the exact same thing with language models so you can go to this website you enter some question you get responses from two models and you don't know what models they were generated from and you pick the winner and then um depending on who wins and who loses you can calculate the ELO scores so the higher the better so what you see here is that crowding up on the top you have the proprietary models these are closed models you don't have access to the weights they are usually behind a web interface and this is gptc from open Ai and the cloud series from anthropic and there's a few other series from other companies as well so these are currently the best performing models and then right below that you are going to start to see some models that are open weights so these weights are available a lot more is known about them there are typically papers available with them and so this is for example the case for llama 2 Series from meta or on the bottom you see Zephyr 7B beta that is based on the mistol series from another startup in France but roughly speaking what you're seeing today in the ecosystem system is that the closed models work a lot better\",\n",
       "  'smry_text': 'Alright, this sounds like an intriguing journey with these language models! Let me break down the key points of what you just explained:\\n\\n**1. Meta\\'s Two-Stage Process (aka  Assistant Models) for Language Generation**\\n\\n* **Stage 1:**  Meta builds \"GPT-like\" language model using AI algorithms and training data. This generates initial, raw outputs we can call them “answers,” that need to be refined.\\n* **Stage 2:** Meta\\'s system focuses on *comparison labeling*. Human reviewers look at the stage 1 answers (generated by GPT or a model), choose which one is best, give scores, and provide guidance to further refine the models. This process enhances accuracy and allows for fine-tuning.\\n\\n\\n**2. Stage Three:  Reinforcement Learning From Human Feedback (RLHF)**\\n* A powerful technique called **RHF (Reinforcement Learning from Human Feedback)** uses human-labeled data to further tweak and align our model\\'s outputs with human preferences. \\n\\n**3. Labeling and Comparison - Examples of the Labeling Instructions:**\\n* You mentioned labeling instructions that are often extensive, complex, and require careful consideration. \\n  \\n **4. A Move Towards Collaborative Human-AI Work**\\n   * The use of language models for tasks like sampling answers, checking work, creating comparisons helps bridge gaps between human capabilities and model performance.\\n\\n**5. Leaderboard: Evaluating Efficiency** \\n* Language models are evaluated using a platform like ChatGPT Arena, where users can generate queries and compare responses from various models.  The ranking system uses ELO-style techniques to determine which model performs best in competitions based on prediction accuracy.\\n\\n\\n **Summarizing the main points:**\\n\\n* We start with advanced language models (both GPT-like, proprietary, and open-weight) built using a two-stage process (first answers followed by human comparison & fine-tuning, often using AI). \\n* This initial model stage uses an extensive, collaborative labeling process for fine-tuning based on comparison tasks for better accuracy. \\n\\n\\nI hope this comprehensive breakdown helps! \\nLet me know if you have any more questions! '},\n",
       " {'uid': 'zjkBMFhNj_g__B4__S540.32',\n",
       "  'text': \" now how do we actually use these neural networks well once we've trained them I showed you that the model inference um is a very simple process we basically generate uh what comes next we sample from the model so we pick a word um and then we continue feeding it back in and get the next word and continue feeding that back in so we can iterate this process and this network then dreams internet documents so for example if we just run the neural network or as we say perform inference uh we would get sort of like web page dreams you can almost think about it that way right because this network was trained on web pages and then you can sort of like Let it Loose so on the left we have some kind of a Java code dream it looks like in the middle we have some kind of a what looks like almost like an Amazon product dream um and on the right we have something that almost looks like Wikipedia article focusing for a bit on the middle one as an example the title the author the ISBN number everything else this is all just totally made up by the network uh the network is dreaming text uh from the distribution that it was trained on it's it's just mimicking these documents but this is all kind of like hallucinated so for example the ISBN number this number probably I would guess almost certainly does not exist uh the model Network just knows that what comes after ISB and colon is some kind of a number of roughly this length and it's got all these digits and it just like puts it in it just kind of like puts in whatever looks reasonable so it's parting the training data set Distribution on the right the black nose days I looked at up and it is actually a kind of fish um and what's Happening Here is this text verbatim is not found in a training set documents but this information if you actually look it up is actually roughly correct with respect to this fish and so the network has knowledge about this fish it knows a lot about this fish it's not going to exactly parrot the documents that it saw in the training set but again it's some kind of a l some kind of a lossy compression of the internet it kind of remembers the gal it kind of knows the knowledge and it just kind of like goes and it creates the form it creates kind of like the correct form and fills it with some of its knowledge and you're never 100% sure if what it comes up with is as we call hallucination or like an incorrect answer or like a correct answer necessarily so some of the stuff could be memorized and some of it is not memorized and you don't exactly know which is which um but for the most part this is just kind of like hallucinating or like dreaming internet text from its data distribution okay let's now switch gears to how does this network work how does it actually perform this next word prediction task what goes on inside it well this is where things complicate a little bit this is kind of like the schematic diagram of the neural network um if we kind of like zoom in into the toy diagram of this neural net this is what we call the Transformer neural network architecture and this is kind of like a diagram of it now what's remarkable about these neural nuts is we actually understand uh in full detail the architecture we know exactly what mathematical operations happen at all the different stages of it uh the problem is that these 100 billion parameters are dispersed throughout the entire neural network work and so basically these buildon parameters uh of billions of parameters are throughout the neural nut and all we know is how to adjust these parameters iteratively to make the network as a whole better at the next word prediction task so we know how to optimize these parameters we know how to adjust them over time to get a better next word prediction but we don't actually really know what these 100 billion parameters are doing we can measure that it's getting better at the next word prediction but we don't know how these parameters collaborate to actually perform that um we have some kind of models that you can try to think through on a high level for what the network might be doing so we kind of understand that they build and maintain some kind of a knowledge database but even this knowledge database is very strange and imperfect and weird uh so a recent viral example is what we call the reversal course uh so as an example if you go to chat GPT and you talk to GPT 4 the best language model currently available you say who is Tom Cruz's mother it will tell you it's merily feifer which is correct but if you say who is merely Fifer's son it will tell you it doesn't know so this knowledge is weird and it's kind of one-dimensional and you have to sort of like this knowledge isn't just like stored and can be accessed in all the different ways you have sort of like ask it from a certain direction almost um and so that's really weird and strange and fundamentally we don't really know because all you can kind of measure is whether it works or not and with what probability so long story short think of llms as kind of like most mostly inscrutable artifacts they're not similar to anything else you might might built in an engineering discipline like they're not like a car where we sort of understand all the parts um there are these neural Nets that come from a long process of optimization and so we don't currently understand exactly how they work although there's a field called interpretability or or mechanistic interpretability trying to kind of go in and try to figure out like what all the parts of this neural net are doing and you can do that to some extent but not fully right now U but right now we kind of what treat them mostly As empirical artifacts we can give them some inputs and we can measure the outputs we can basically measure their behavior we can look at the text that they generate in many different situations and so uh I think this requires basically correspondingly sophisticated evaluations to work with these models because they're mostly empirical so now let's go to how we actually obtain an assistant so far we've only talked about these internet document generators right um and so that's the first stage of training we call that stage pre-training we're now moving to the second stage of training which we call fine-tuning and this is where we obtain what we call an assistant model because we don't actually really just want a document generators that's not very helpful for many tasks we want um to give questions to something and we want it to generate answers based on those questions so we really want an assistant model instead and the way you obtain these assistant models is fundamentally uh through the following process we basically keep the optimization identical so the training will be the same it's just the next word prediction task but we're going to s swap out the data set on which we are training so it used to be that we are trying to uh train on internet documents we're going to now swap it out for data sets that we collect manually and the way we collect them is by using lots of people so typically a company will hire people and they will give them labeling instructions and they will ask people to come up with questions and then write answers for them so here's an example of a single example um that might basically make it into your training set so there's a user and uh it says something like can you write a short introduction about the relevance of the term monopsony in economics and so on and then there's assistant and again the person fills in what the ideal response should be and the ideal response and how that is specified and what it should look like all just comes from labeling documentations that we provide these people and the engineers at a company like open or anthropic or whatever else will come up with these labeling documentations now the pre-training stage is about a large quantity of text but potentially low quality because it just comes from the internet and there's tens of or hundreds of terabyte Tech off it and it's not all very high qu uh qu quality but in this second stage uh we prefer quality over quantity so we may have many fewer documents for example 100,000 but all these documents now are conversations and they should be very high quality conversations and fundamentally people create them based on abling instructions so we swap out the data set now and we train on these Q&A documents we uh and this process is called fine tuning once you do this you obtain what we call an assistant model so this assistant model now subscribes to the form of its new training documents so for example if you give it a question like can you help me with this code it seems like there's a bug print Hello World um even though this question specifically was not part of the training Set uh the model after its fine-tuning understands that it should answer in the style of a helpful assistant to these kinds of questions and it will do that so it will sample word by word again from left to right from top to bottom all these words that are the response to this query and so it's kind of remarkable and also kind of empirical and not fully understood that these models are able to sort of like change their formatting into now being helpful assistants because they've seen so many documents of it in the fine chaining stage but they're still able to access and somehow utilize all the knowledge that was built up during the first stage the pre-training stage so roughly speaking pre-training stage is um training on trains on a ton of internet and it's about knowledge and the fine truning stage is about what we call alignment it's about uh sort of giving um it's a it's about like changing the formatting from internet documents to question and answer documents in kind of like a helpful assistant manner so roughly speaking here are the two major parts of obtaining something like chpt there's the stage one pre-training and stage two fine-tuning in the pre-training stage you get a ton of text from the internet you need a cluster of gpus so these are special purpose uh sort of uh computers for these kinds of um parel processing workloads this is not just things that you can buy and Best Buy uh these are very expensive computers and then you compress the text into this neural network into the parameters of it uh typically this could be a few uh sort of millions of dollars um and then this gives you the base model because this is a very computationally expensive part this only happens inside companies maybe once a year or once after multiple months because this is kind of like very expens very expensive to actually perform once you have the base model you enter the fing stage which is computationally a lot cheaper in this stage you write out some labeling instru instructions that basically specify how your assistant should behave then you hire people um so for example scale AI is a company that actually would um uh would work with you\",\n",
       "  'smry_text': 'Here\\'s a breakdown of how AI model assistants, like ChatGPT, are created.\\n\\n**1. Pre-training: Building the Foundation (Large Data Set)**\\n\\n* **Data Collection:** A massive amount of text data is scraped from the internet—books, articles, websites, social media posts.  Think hundreds of terabytes!\\n* **Goal:** This initial training stage involves teaching the model to understand the vast array of human language patterns, grammar, common expressions, and even subtle nuances within the raw data. Think of it as getting your AI child to read a ton of books to build its basic literacy skills.\\n* **Challenge:** Finding high-quality training data is crucial.  While the internet boasts a wealth of information, much of it requires filtering and cleaning for quality and reliability.\\n\\n**2. Fine-tuning: Adapting to User Interaction (High-Quality Q&A Set)**\\n\\n* **Data Collection:** Carefully curated question-and-answer dataset focusing on conversational styles. \\n    * Example: \"Can you help me with this code? It seems there\\'s a bug.\" \\n* **Goal:** This stage refines the model\\'s abilities to specifically follow a dialogue format. Each answer needs to be a helpful and relevant response to user prompts.  Think of it as learning how to give precise, coherent answers that suit casual conversation. \\n* **Human Input:** Humans play a vital role defining specific ways the AI assistant should respond, ensuring the overall consistency is maintained.  \\n\\n**Key Elements in Fine-Tuning**\\n\\n* **Alignment: ** This is the crucial aspect that differentiates fine-tuning from pre-training.  In this stage, we go beyond just patterns and teach the model how to be conversational (like teaching language skills). It\\'s about adjusting its style and approach for specific tasks or conversational prompts. \\n    * Think of it as teaching your chatbot how to become a fluent and helpful conversational AI!\\n* **Labeling Instructions:** These detailed guidelines (written and annotated by experts, like human trainers) show the model how answers should be structured based on pre-defined topics, rules, and styles\\n\\n**3. The Result:  A More Conversational & Useful AI Assistant**\\n\\nFine-tuning transforms the model from a general language learner to a chatbot that understands conversational cues (context, tone) and can provide user-requested information in a more natural and helpful way.\\n\\n**The Role of Companies in Fine-Tuning:**\\n\\n\\n* **Companies like OpenAI (ChatGPT), Google**, and **Anthropic**, etc., actively collect and label questions to train these assistant models, employing human specialists for guidance on the quality of responses and consistency.  They need to create a vast collection of high-quality question-and-answer pairs that cover a wide range of topics.\\n\\n\\n**In Summary:** The key lies in striking a balance between acquiring knowledge from the world and training it towards specific conversational skills which is what fine-tuning effectively does to produce an AI assistant model ready for a range of user interaction. \\n'},\n",
       " {'uid': 'zjkBMFhNj_g__B5__S720.519',\n",
       "  'text': \" basically these buildon parameters uh of billions of parameters are throughout the neural nut and all we know is how to adjust these parameters iteratively to make the network as a whole better at the next word prediction task so we know how to optimize these parameters we know how to adjust them over time to get a better next word prediction but we don't actually really know what these 100 billion parameters are doing we can measure that it's getting better at the next word prediction but we don't know how these parameters collaborate to actually perform that um we have some kind of models that you can try to think through on a high level for what the network might be doing so we kind of understand that they build and maintain some kind of a knowledge database but even this knowledge database is very strange and imperfect and weird uh so a recent viral example is what we call the reversal course uh so as an example if you go to chat GPT and you talk to GPT 4 the best language model currently available you say who is Tom Cruz's mother it will tell you it's merily feifer which is correct but if you say who is merely Fifer's son it will tell you it doesn't know so this knowledge is weird and it's kind of one-dimensional and you have to sort of like this knowledge isn't just like stored and can be accessed in all the different ways you have sort of like ask it from a certain direction almost um and so that's really weird and strange and fundamentally we don't really know because all you can kind of measure is whether it works or not and with what probability so long story short think of llms as kind of like most mostly inscrutable artifacts they're not similar to anything else you might might built in an engineering discipline like they're not like a car where we sort of understand all the parts um there are these neural Nets that come from a long process of optimization and so we don't currently understand exactly how they work although there's a field called interpretability or or mechanistic interpretability trying to kind of go in and try to figure out like what all the parts of this neural net are doing and you can do that to some extent but not fully right now U but right now we kind of what treat them mostly As empirical artifacts we can give them some inputs and we can measure the outputs we can basically measure their behavior we can look at the text that they generate in many different situations and so uh I think this requires basically correspondingly sophisticated evaluations to work with these models because they're mostly empirical so now let's go to how we actually obtain an assistant so far we've only talked about these internet document generators right um and so that's the first stage of training we call that stage pre-training we're now moving to the second stage of training which we call fine-tuning and this is where we obtain what we call an assistant model because we don't actually really just want a document generators that's not very helpful for many tasks we want um to give questions to something and we want it to generate answers based on those questions so we really want an assistant model instead and the way you obtain these assistant models is fundamentally uh through the following process we basically keep the optimization identical so the training will be the same it's just the next word prediction task but we're going to s swap out the data set on which we are training so it used to be that we are trying to uh train on internet documents we're going to now swap it out for data sets that we collect manually and the way we collect them is by using lots of people so typically a company will hire people and they will give them labeling instructions and they will ask people to come up with questions and then write answers for them so here's an example of a single example um that might basically make it into your training set so there's a user and uh it says something like can you write a short introduction about the relevance of the term monopsony in economics and so on and then there's assistant and again the person fills in what the ideal response should be and the ideal response and how that is specified and what it should look like all just comes from labeling documentations that we provide these people and the engineers at a company like open or anthropic or whatever else will come up with these labeling documentations now the pre-training stage is about a large quantity of text but potentially low quality because it just comes from the internet and there's tens of or hundreds of terabyte Tech off it and it's not all very high qu uh qu quality but in this second stage uh we prefer quality over quantity so we may have many fewer documents for example 100,000 but all these documents now are conversations and they should be very high quality conversations and fundamentally people create them based on abling instructions so we swap out the data set now and we train on these Q&A documents we uh and this process is called fine tuning once you do this you obtain what we call an assistant model so this assistant model now subscribes to the form of its new training documents so for example if you give it a question like can you help me with this code it seems like there's a bug print Hello World um even though this question specifically was not part of the training Set uh the model after its fine-tuning understands that it should answer in the style of a helpful assistant to these kinds of questions and it will do that so it will sample word by word again from left to right from top to bottom all these words that are the response to this query and so it's kind of remarkable and also kind of empirical and not fully understood that these models are able to sort of like change their formatting into now being helpful assistants because they've seen so many documents of it in the fine chaining stage but they're still able to access and somehow utilize all the knowledge that was built up during the first stage the pre-training stage so roughly speaking pre-training stage is um training on trains on a ton of internet and it's about knowledge and the fine truning stage is about what we call alignment it's about uh sort of giving um it's a it's about like changing the formatting from internet documents to question and answer documents in kind of like a helpful assistant manner so roughly speaking here are the two major parts of obtaining something like chpt there's the stage one pre-training and stage two fine-tuning in the pre-training stage you get a ton of text from the internet you need a cluster of gpus so these are special purpose uh sort of uh computers for these kinds of um parel processing workloads this is not just things that you can buy and Best Buy uh these are very expensive computers and then you compress the text into this neural network into the parameters of it uh typically this could be a few uh sort of millions of dollars um and then this gives you the base model because this is a very computationally expensive part this only happens inside companies maybe once a year or once after multiple months because this is kind of like very expens very expensive to actually perform once you have the base model you enter the fing stage which is computationally a lot cheaper in this stage you write out some labeling instru instructions that basically specify how your assistant should behave then you hire people um so for example scale AI is a company that actually would um uh would work with you to actually um basically create documents according to your labeling instructions you collect 100,000 um as an example high quality ideal Q&A responses and then you would fine-tune the base model on this data this is a lot cheaper this would only potentially take like one day or something like that instead of a few uh months or something like that and you obtain what we call an assistant model then you run a lot of Valu ation you deploy this um and you monitor collect misbehaviors and for every misbehavior you want to fix it and you go to step on and repeat and the way you fix the Mis behaviors roughly speaking is you have some kind of a conversation where the Assistant gave an incorrect response so you take that and you ask a person to fill in the correct response and so the the person overwrites the response with the correct one and this is then inserted as an example into your training data and the next time you do the fine training stage uh the model will improve in that situation so that's the iterative process by which you improve this because fine tuning is a lot cheaper you can do this every week every day or so on um and companies often will iterate a lot faster on the fine training stage instead of the pre-training stage one other thing to point out is for example I mentioned the Llama 2 series The Llama 2 Series actually when it was released by meta contains contains both the base models and the assistant models so they release both of those types the base model is not directly usable because it doesn't answer questions with answers uh it will if you give it questions it will just give you more questions or it will do something like that because it's just an internet document sampler so these are not super helpful where they are helpful is that meta has done the very expensive part of these two stages they've done the stage one and they've given you the result and so you can go off and you can do your own fine-tuning uh and that gives you a ton of Freedom um but meta in addition has also released assistant models so if you just like to have a question answer uh you can use that assistant model and you can talk to it okay so those are the two major stages now see how in stage two I'm saying end or comparisons I would like to briefly double click on that because there's also a stage three of fine tuning that you can optionally go to or continue to in stage three of fine tuning you would use comparison labels uh so let me show you what this looks like the reason that we do this is that in many cases it is much easier to compare candidate answers than to write an answer yourself if you're a human labeler so consider the following concrete example suppose that the question is to write a ha cou about paper clips or something like that uh from the perspective of a labeler if I'm asked to write a ha cou that might be a very difficult task right like I might not be able to write a Hau but suppose you're given a few candidate Haus that have been generated by the assistant model from stage two well then as a labeler you could look at these Haus and actually pick the one that is much better and so in many cases it is easier to do the comparison instead of the\",\n",
       "  'smry_text': 'Great explanation! This breakdown covers **pre-training**, **fine-tuning**, and how they work together to create helpful AI assistants like ChatGPT.  Let\\'s break down some key points for even more clarity:\\n\\n**Pre-Training (Stage One):**\\n\\n* **The Heavy Lifter:** Imagine a child learning their first language by reading thousands of books. Pre-training gives the model a broad understanding from internet data (books, articles, code, everything!).\\n* **Resource Intensive:** This \"feeding\" the model vast amounts of data requires clusters of powerful GPUs and significant computational resources - making pre-training an expensive endeavor. \\n\\n**Fine-Tuning (Stage Two):**\\n\\n* **The Clarifier:** After a broad understanding, fine-tuning refines the model to act as a helpful \"assistant\" for specific tasks (writing essays, answering questions). It takes place by comparing desired responses with the output from pre-training.\\n* **Low Cost Approach**: Fine-tuning is much cheaper than pre-training and can be done more frequently - key for iterative improvements of the assistant\\'s abilities. \\n\\n**Comparison Labels (Stage Three):**\\n\\n* **Human Expertise Amplifies:** This stage relies heavily on human feedback to judge the accuracy and quality of responses from the fine-tuned model. It helps refine the model\\'s output.  Examples include:\\n    * Comparing generated answers with human-written examples.      \\n    * Identifying \"better\" response options from pre-processed data.\\n\\n\\n**Key Differences Between ChatGPT and Traditional Models:**\\n\\n* **ChatGPT\\'s Focus:** It aims for a conversational AI style, providing responses within context,  answering questions directly to a user, and generating creative text formats (like songs or emails).\\n\\n\\nTo summarize: Pre-training provides a vast knowledge base; fine-tuning sharpens its ability for dialogue tasks. Comparison labels enable the refinement of conversational quality through expert human evaluation. \\n\\n\\n\\nDoes this make things clearer? Please let me know if you would like further exploration of specific details! \\n'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test run for text search\n",
    "query = 'How can we finetune an LLM to work as an assistant?'\n",
    "elastic_search_txt(query, index_name, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    '''\n",
    "    This function creates a prompt using provided 'search_results' that can be used to generate llm response for the user provided 'query'.\n",
    "    '''\n",
    "    prompt_template = \"\"\"\n",
    "        You're provided a summarized youtube video transcript. Answer the QUESTION based on the CONTEXT which is the summarized transcript.\n",
    "        Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "        QUESTION: {question}\n",
    "\n",
    "        CONTEXT: \n",
    "        {context}\n",
    "        \"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"{doc['smry_text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, llm_model='gemma2:2b'):\n",
    "    '''\n",
    "    This function uses 'llm_model' to generate response for the provided input 'prompt' to llm.\n",
    "    '''\n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        # temperature=0,    # remove randomness for deterministic output but not using it as it makes summary clumsy with phrases like 'you stated correctly...', 'you explained it well...' etc.\n",
    "        seed=72\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = elastic_search_txt(query, index_name, 4)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'error': {'message': 'model requires more system memory (3.3 GiB) than is available (1.6 GiB)', 'type': 'api_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# test run the text search rag\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mrag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[56], line 4\u001b[0m, in \u001b[0;36mrag\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      2\u001b[0m search_results \u001b[38;5;241m=\u001b[39m elastic_search_txt(query, index_name, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m build_prompt(query, search_results)\n\u001b[1;32m----> 4\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "Cell \u001b[1;32mIn[55], line 5\u001b[0m, in \u001b[0;36mllm\u001b[1;34m(prompt, llm_model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllm\u001b[39m(prompt, llm_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgemma2:2b\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    This function uses 'llm_model' to generate response for the provided input 'prompt' to llm.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# temperature=0,    # remove randomness for deterministic output but not using it as it makes summary clumsy with phrases like 'you stated correctly...', 'you explained it well...' etc.\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m72\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32md:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:742\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    739\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    740\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    741\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    766\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\openai\\_base_client.py:1270\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1258\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1267\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1268\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1269\u001b[0m     )\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\openai\\_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    945\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\openai\\_base_client.py:1036\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1035\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\openai\\_base_client.py:1085\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\openai\\_base_client.py:1036\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1035\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32md:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\openai\\_base_client.py:1085\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\openai\\_base_client.py:1051\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1050\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1054\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1055\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1059\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1060\u001b[0m )\n",
      "\u001b[1;31mInternalServerError\u001b[0m: Error code: 500 - {'error': {'message': 'model requires more system memory (3.3 GiB) than is available (1.6 GiB)', 'type': 'api_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "# test run the text search rag\n",
    "rag(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
