{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = 'zjkBMFhNj_g'\n",
    "srt = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "df_srt = pd.DataFrame(srt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_transcript(df):\n",
    "    out_text =\"\"\n",
    "    for _, row in df.iterrows():\n",
    "        out_text += \" \" + row['text']\n",
    "    return out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64353,\n",
       " 12151,\n",
       " ' hi everyone so recently I gave a 30-minute talk on large language models just kind of like an intro talk um unfortunately that talk was not recorded but a lot of people came to me after the talk and ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt_transcribed = parse_transcript(df_srt)\n",
    "len(yt_transcribed), len(yt_transcribed.split()), yt_transcribed[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking | Create Input Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blocks(df, block_size=5, stride=1, max_duration=120):\n",
    "    '''\n",
    "    Use sliding window of size 'block_size' minutes with stride of 'stride' minutes to generate text blocks.\n",
    "    Generated blocks wil be limited to 'max_blocks' and can be changed depending upon the processing power.\n",
    "    Default parameters allow videos of upto 2hrs. to be included.\n",
    "    '''\n",
    "    max_blocks = ceil(((max_duration-block_size)/stride)+1)\n",
    "    max_len = ceil(max(df['start'])/60)\n",
    "    df_out = pd.DataFrame()\n",
    "\n",
    "    print(f\"INFO: initiated block creation of video transcript\")\n",
    "    print(f\"INFO: video length {max_len} | block size {block_size} | stride {stride} | max blocks {max_blocks}\")\n",
    "\n",
    "    for i in range(max_blocks):\n",
    "        start = i*stride\n",
    "        stop = block_size + i*stride\n",
    "        df_block = df[(df['start']>= 60*start) & (df['start']<= 60*stop)]\n",
    "        if (i + 1) % 5 == 0 or i + 1 == max_blocks:\n",
    "            print(f\"INFO: generated block {i+1} | start {start} | stop {stop} | rows combined {df_block.shape[0]}\")\n",
    "            print(f\"INFO: reached max blocks limit\")\n",
    "        transcribed = parse_transcript(df_block)\n",
    "        df_block = pd.DataFrame({'Block':[i+1], 'text':[transcribed], 'start_time': [min(df_block['start'])]})\n",
    "        df_out = pd.concat([df_out, df_block])\n",
    "        if stop >= max_len:\n",
    "            print(f\"INFO: generated block {i+1} | start {start} | stop {stop} | rows combined {df_block.shape[0]}\")\n",
    "            print(f\"INFO: reached end of video\")\n",
    "            break\n",
    "    \n",
    "    df_out.reset_index(drop=True, inplace=True)\n",
    "    print(f\"INFO: original data {df.shape} | block data {df_out.shape}\")\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: initiated block creation of video transcript\n",
      "INFO: video length 60 | block size 5 | stride 1 | max blocks 116\n",
      "INFO: generated block 5 | start 4 | stop 9 | rows combined 140\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 10 | start 9 | stop 14 | rows combined 150\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 15 | start 14 | stop 19 | rows combined 140\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 20 | start 19 | stop 24 | rows combined 143\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 25 | start 24 | stop 29 | rows combined 143\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 30 | start 29 | stop 34 | rows combined 139\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 35 | start 34 | stop 39 | rows combined 149\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 40 | start 39 | stop 44 | rows combined 146\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 45 | start 44 | stop 49 | rows combined 137\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 50 | start 49 | stop 54 | rows combined 142\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 55 | start 54 | stop 59 | rows combined 138\n",
      "INFO: reached max blocks limit\n",
      "INFO: generated block 56 | start 55 | stop 60 | rows combined 1\n",
      "INFO: reached end of video\n",
      "INFO: original data (1704, 3) | block data (56, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Block</th>\n",
       "      <th>text</th>\n",
       "      <th>start_time</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hi everyone so recently I gave a 30-minute ta...</td>\n",
       "      <td>0.160</td>\n",
       "      <td>5356</td>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>biggest one now many people like this model s...</td>\n",
       "      <td>60.039</td>\n",
       "      <td>5378</td>\n",
       "      <td>1021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Block                                               text  start_time  \\\n",
       "0      1   hi everyone so recently I gave a 30-minute ta...       0.160   \n",
       "1      2   biggest one now many people like this model s...      60.039   \n",
       "\n",
       "   text_length  word_count  \n",
       "0         5356        1015  \n",
       "1         5378        1021  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_srt_chunks = create_blocks(df_srt)\n",
    "\n",
    "df_srt_chunks['text_length'] = df_srt_chunks['text'].apply(len)\n",
    "df_srt_chunks['word_count'] = df_srt_chunks['text'].apply(lambda x : len(x.split()))\n",
    "df_srt_chunks.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Clean Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on windows in local, use line below to change hf model download location\n",
    "hf_cache_dir = \"D:\\.cache\\huggingface\"\n",
    "os.environ['HF_HOME'] = \"D:\\.cache\\huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc9d6cb813248008d1bb969aaa4090b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Shashank Prakash\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4876cce43c3d49aeb68bdea77e1e67b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32970c469c9b4d1ca7408872939e28f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88dc706af0474ae28d1308f072d63f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819f9fb78dbe4534b85eada78794f986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed53f74c17954afe87ff4365e299bf2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\", )\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Installed Programs\\miniconda3\\envs\\vidsage\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_srt_chunks.loc[0, 'text'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"A large language model is just two files. The parameters are basically the weights or the parameters of this neural network that is the language model. Every one of those parameters is stored as 2 bytes and so therefore the parameters file here is 140 gigabytes and it's two bytes because this is a float 16 as the data type. This is a fully self-contained package this is everything that's necessary you don't need any connectivity to the internet or anything else you can take these two files you compile your C code you get a binary that you can point at the parameters and you can talk to this language model so for example you can send it text like for example write a poem about the company scale Ai and this model will start generating text and in this case it will follow the directions and give you a poems about scale AI. This talk was not actually in terms of the speed of this uh video here this was not running a 70 billion parameter model it was only running a 7 billion parameter Model A 70b would be running about 10 times slower but I wanted to give you an idea of just the text generation and what that looks like. The reason that I'm picking on scale AI here and you're going to see that throughout the talk is because the event that I originally presented uh this talk with was run by scale Ai. I'm pick on them throughout uh throughout the slides a little bit just in an effort to make it concrete so this is how we can run the model just requires two files just requires a MacBook. It would only require about 500 lines of C with no other dependencies to implement the the neural network architecture.\"}]\n"
     ]
    }
   ],
   "source": [
    "article = \" \".join(df_srt_chunks.loc[0, 'text'].split()[:950])\n",
    "print(summarizer(article, do_sample=False, min_length=300, max_length= 600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vidsage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
